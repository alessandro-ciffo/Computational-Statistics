{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to find a local minimum in a differentiable function. To find such local minimum, we take steps proportionals to the negative of the gradient at the current point.\n",
    "\n",
    "More formally, let $F(\\textbf{x})$ denote a multivariable function. The algorithm goes as follows:\n",
    "- Starts with an inital guess $\\textbf {x}_0$. \n",
    "- For each next step in the sequence $\\textbf {x}_1, \\textbf {x}_2, \\textbf {x}_3, ...$ we set:\n",
    "$$\\textbf {x}_{n+1} = \\textbf {x}_n - \\gamma \\cdot \\nabla F(\\textbf{x}_n)$$\n",
    "\n",
    "where $\\gamma$ is the *step size* and $\\nabla F(\\textbf{x}_n)$ is the *gradient* of $F(\\textbf{x})$ at point $\\textbf{x}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Algorithm\n",
    "\n",
    "First of all we are going to need a function that computes a numerical approximation of the gradient at a given point. In particular the gradient $\\nabla F(\\textbf{x})$ is the vector containing the partial derivatives for each component $\\textbf{x}$. The partial derivatives for component *i* is defined as:\n",
    "\n",
    "$$\\frac{\\partial F}{\\partial x_i} = \\lim_{\\epsilon \\to 0} \\frac{F(x_1,...,x_i + \\epsilon,..., x_n) - F(x_1,...,x_i,..., x_n)}{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "def grad(f, x, eps=1e-10):\n",
    "    \"\"\"Compute gradient of a function at a given point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function\n",
    "        The function for which to compute the gradient\n",
    "    x : ndarray\n",
    "        Point at which the gradient must be computed\n",
    "    eps : float\n",
    "        Small change in a given direction\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    g : ndarray\n",
    "        Numpy array containing the gradient of f at point x\n",
    "    \"\"\"\n",
    "    \n",
    "    g = x.copy()\n",
    "    x2 = x.copy()\n",
    "    f0 = f(x)\n",
    "    for i in range(len(x)):\n",
    "        x2[i] += eps\n",
    "        g[i] = (f(x2) - f0) / eps\n",
    "        x2[i] = x[i]\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement hgradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeGD(f, x, gamma=1e-3, tmax=100000, infotime=100, eps=1e-10):\n",
    "    \"\"\"Optimize function f using Gradient Descent\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function\n",
    "        Function to be optimized\n",
    "    x : ndarray\n",
    "        Initial guess for the local minimum \n",
    "    gamma : float\n",
    "        Step size\n",
    "    tmax : int\n",
    "        Maximum number of iterations\n",
    "    infotime : int\n",
    "        Print iteration number and value of the function each infotime steps\n",
    "    eps : float\n",
    "        If the displacement norm is smaller than eps the algorithm stops\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : ndarray\n",
    "        Proposed local minimum of f\n",
    "    f(x) : float\n",
    "        Value of the function at x\n",
    "    \"\"\"\n",
    "    \n",
    "    ts = []\n",
    "    losses = []\n",
    "    \n",
    "    for t in range(tmax):\n",
    "        g = grad(f, x)\n",
    "        xnew = x - gamma * g\n",
    "        delta = np.linalg.norm(x - xnew)\n",
    "        x = xnew\n",
    "        \n",
    "        if t % infotime == 0:\n",
    "            ts.append(t)\n",
    "            losses.append(f(x))\n",
    "            print(f\"t={t}   Loss={losses[-1]}\")\n",
    "    \n",
    "        if delta < eps:\n",
    "            print(f\"Converged at iteration {t} at x:{x} loss:{f(x)}\")\n",
    "            break\n",
    "    \n",
    "    return x, f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Rosenbrock function\n",
    "\n",
    "Let's test the algorithm on the well-known Rosenbrock function\n",
    "\n",
    "$$f(x, y) = (a - x)^2 + b(y - x^2)^2$$\n",
    "\n",
    "This function is often used as a performance test for optimization algorithms. Usually a is set to 1 and b is set to 100 and we know that for those two values, the function has a global minimum at $x = (1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0   Loss=256.8389911822251\n",
      "t=1000   Loss=0.08021934341312033\n",
      "t=2000   Loss=0.025373829586419358\n",
      "t=3000   Loss=0.00948568759438832\n",
      "t=4000   Loss=0.0038262831868872507\n",
      "t=5000   Loss=0.0016084950209828224\n",
      "t=6000   Loss=0.0006928914968362198\n",
      "t=7000   Loss=0.00030302783136777994\n",
      "t=8000   Loss=0.00013381040104850627\n",
      "t=9000   Loss=5.945883327699298e-05\n",
      "t=10000   Loss=2.652941606098097e-05\n",
      "t=11000   Loss=1.1869143790731549e-05\n",
      "t=12000   Loss=5.319807288378153e-06\n",
      "t=13000   Loss=2.387241960710876e-06\n",
      "t=14000   Loss=1.0721324957787366e-06\n",
      "t=15000   Loss=4.817678210165564e-07\n",
      "t=16000   Loss=2.1656552650425606e-07\n",
      "t=17000   Loss=9.737657909683706e-08\n",
      "t=18000   Loss=4.379286046276421e-08\n",
      "t=19000   Loss=1.96978693965665e-08\n",
      "t=20000   Loss=8.861285996682257e-09\n",
      "t=21000   Loss=3.986944458314642e-09\n",
      "t=22000   Loss=1.794174082047196e-09\n",
      "t=23000   Loss=8.076031768236321e-10\n",
      "t=24000   Loss=3.636521342248526e-10\n",
      "t=25000   Loss=1.6383226775084949e-10\n",
      "t=26000   Loss=7.386597642526935e-11\n",
      "t=27000   Loss=3.3341142722274806e-11\n",
      "t=28000   Loss=1.507455450286604e-11\n",
      "t=29000   Loss=6.832617257796129e-12\n",
      "t=30000   Loss=3.108311707631929e-12\n",
      "t=31000   Loss=1.42171278861803e-12\n",
      "t=32000   Loss=6.554542900499306e-13\n",
      "t=33000   Loss=3.0568731226726084e-13\n",
      "t=34000   Loss=1.4494401567053813e-13\n",
      "t=35000   Loss=7.035147146001984e-14\n",
      "t=36000   Loss=3.526475742406409e-14\n",
      "Converged at iteration 36861 at x:[0.99999986 0.99999972] loss:2.0131236317767887e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.99999986, 0.99999972]), 2.0131236317767887e-14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rosenbrock(x, a=1, b=100):\n",
    "    return (a-x[0])**2 + b*(x[1]-x[0]**2)**2\n",
    "\n",
    "# Starting point\n",
    "x = np.array([0.01, 2], dtype=np.float64)\n",
    "\n",
    "optimizeGD(rosenbrock, x, infotime=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above, the algorithm achieves the desired global minimum $x = (1,1)$ at which $f(x) = 0$, hence we managed to optimize the function.\n",
    "\n",
    "The main problems with simple Gradient Descent are that the algorithm may converge too slowly or that choosing a step size that is too large may lead to divergence. To tackle these problems, other methods have been developed, such as Fast Gradient Methods and Momentum Methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
